# File: infra/k8s/charts/inference-server/Chart.yaml
apiVersion: v2
name: inference-server
description: A Helm chart for deploying the QuantumArb 2.0 ML Inference Server.
type: application
version: 0.1.0
appVersion: "1.0.0"

---
# File: infra/k8s/charts/inference-server/values.yaml
replicaCount: 2

image:
  repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/inference-server
  pullPolicy: IfNotPresent
  tag: "latest"

service:
  type: ClusterIP
  port: 80

# ML inference can be CPU-intensive.
resources:
  limits:
    cpu: "1"
    memory: "1Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"

# Standard Helm boilerplate
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  create: true
  annotations: {}
  name: ""
podAnnotations: {}
podSecurityContext: {}
securityContext: {}
nodeSelector: {}
tolerations: []
affinity: {}

---
# File: infra/k8s/charts/inference-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "inference-server.fullname" . }}
  labels:
    {{- include "inference-server.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "inference-server.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "inference-server.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "inference-server.serviceAccountName" . }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          resources:
            {{- toYaml .Values.resources | nindent 12 }}

---
# File: infra/k8s/charts/inference-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "inference-server.fullname" . }}
  labels:
    {{- include "inference-server.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "inference-server.selectorLabels" . | nindent 4 }}

---
# File: infra/k8s/charts/inference-server/templates/_helpers.tpl
{{- define "inference-server.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "inference-server.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{- define "inference-server.labels" -}}
helm.sh/chart: {{ include "inference-server.name" . }}-{{ .Chart.Version | replace "+" "_" }}
{{ include "inference-server.selectorLabels" . }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{- define "inference-server.selectorLabels" -}}
app.kubernetes.io/name: {{ include "inference-server.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{- define "inference-server.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "inference-server.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
