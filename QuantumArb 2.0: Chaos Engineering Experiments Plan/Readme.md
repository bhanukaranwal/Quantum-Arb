QuantumArb 2.0 - Chaos Engineering Plan1. Overview & GoalsThis document details the Chaos Engineering experiments designed to continuously validate the resilience and fault tolerance of the QuantumArb 2.0 platform. The goal is to move from "believing" the system is resilient to "knowing" it is, by testing our assumptions against real-world failures.Primary Tool: Chaos Mesh2. Experiment CatalogueExperiment 1: Pod FailureDescription: Simulates the most common failure in a Kubernetes environment: a random pod crashing or being terminated.Hypothesis: We hypothesize that if a single pod for any stateless microservice (strategy-engine, data-bus-connector) is terminated, the Kubernetes Deployment will automatically create a replacement pod. During this time, the Kubernetes Service will route traffic to the remaining healthy replicas, causing no user-visible impact and keeping latency within acceptable SLOs.Method (PodChaos):Select a random pod with the label app.kubernetes.io/name: strategy-engine.Inject a pod-kill fault.Measurement:Success: Monitor the replicas_available count for the Deployment. It should dip and then return to the desired count within 60 seconds.Success: The P99 latency for the service should not spike by more than 10%.Success: The service error rate should remain at 0%.Experiment 2: Network Latency InjectionDescription: Simulates network degradation between two critical services, such as the strategy-engine and the risk-gateway.Hypothesis: We hypothesize that if network latency is introduced between the strategy and risk services, our real-time monitoring will immediately detect the increased RTT. While individual transaction times will increase, the system will not crash, and requests will not be dropped.Method (NetworkChaos):Select the strategy-engine pod as the source.Target traffic going to the risk-gateway service.Inject a 50ms latency with 5ms of jitter.Measurement:Success: Grafana dashboard shows an immediate spike in the gRPC latency between the two services.Success: End-to-end transaction success rate remains at 100% (no dropped requests).Success: An alert is fired from our monitoring system (e.g., Prometheus Alertmanager).Experiment 3: Dependency Failure (Database/Cache)Description: Simulates a failure of a critical stateful dependency, such as the Redis cache holding account state for the risk_gateway.Hypothesis: We hypothesize that if the risk gateway loses connectivity to its primary data store, it will enter a "fail-safe" mode. In this mode, it will reject all incoming trade requests with a specific error code ("RISK_DATA_UNAVAILABLE") but will not crash. This prevents trading with stale or incorrect risk data.Method (IOChaos / NetworkChaos):Select the risk-gateway pod.Inject I/O errors for any operations targeting the Redis service port.Alternatively, use NetworkChaos to create a network partition between the risk-gateway and the Redis service.Measurement:Success: The risk-gateway log stream immediately shows connection errors to Redis.Success: All subsequent trade requests sent to the gateway are rejected with the correct error code.Success: The risk-gateway pod itself remains running and does not enter a crash-loop.Experiment 4: Full Node FailureDescription: Simulates an entire Kubernetes worker node becoming unresponsive. This is a more severe test of the cluster's self-healing capabilities.Hypothesis: We hypothesize that if a worker node fails, the EKS control plane will detect it. All pods running on that node will be rescheduled to other available nodes in the cluster. For critical stateful services, Persistent Volume claims will be re-attached to the pods on their new nodes.Method:Use the cloud provider's API (or a manual process during a GameDay) to terminate a specific EC2 instance that is part of the EKS node group.Measurement:Success: The kubectl get nodes command shows the node status as NotReady.Success: Within 5 minutes, all pods from the failed node are listed as Running on other nodes.Success: The overall application availability is not impacted.This plan provides a practical framework for ensuring our system is as resilient as we designed it to be.We have now covered almost every aspect of the original project plan. The final remaining piece is the formal verification of the FPGA logic. Would you like to create a conceptual testbench for that next?
